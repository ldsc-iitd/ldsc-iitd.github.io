
<!-- saved from url=(0037)http://cnnlocalization.csail.mit.edu/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>MALL-IITD</title>

<script async="" src="./mall-iitd/analytics.js.download"></script><script type="text/javascript" src="./mall-iitd/jquery.mlens-1.0.min.js.download"></script>
<script type="text/javascript" src="./mall-iitd/jquery.js.download"></script>
<style>
body
{
    font-family : Arial;
	background-color : #111;
}
.content
{
    width : 800px;
    padding : 25px 50px;
    margin : 25px auto;
    background-color : #fff;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
}

.contentblock
{
    width : 950px;
    margin : 0 auto;
    padding : 0;
    border-spacing : 25px 0;
}

.contentblock td
{
    background-color : #fff;
    padding : 25px 50px;
    vertical-align : top;
    box-shadow: 0px 0px 10px #999;
    border-radius: 15px; 
}

a, a:visited
{
    color: #224b8d;
}

#authors
{
    text-align : center;
    margin-bottom : 20px;
}

#conference
{
    text-align : center;
    margin-bottom : 20px;
    font-style : italic;
}

#authors a 
{
    margin : 0 10px;
}

h1
{
    text-align : center;
    font-family : Arial;
    font-size : 20px;
}

code
{
	display : block;
	padding : 10px;
	margin : 10px 10px;
}
p code
{
    display : inline;
    padding : 0;
    margin : 0;
}
#teasers
{
    margin : 0 auto;    
}

#teasers td
{
    margin : 0 auto;
    text-align : center;
    padding : 5px;
}

#teasers img
{
    width : 250px; 
}

#results img
{
    width : 133px;
}

#seeintodark {
    margin : 0 auto;
}

#sift 
{
    margin : 0 auto;
}

#sift img
{
    width : 250px;
}

.downloadpaper 
{
    padding-left : 20px;
    float : right;
    text-align : center;
}

.downloadpaper a 
{
    font-weight : bold;
    text-align : center;
}

#demoframe
{
    border : 0;
    padding : 0;
    margin : 0;
    width : 100%;
    height : 340px;
}

#feedbackform
{
    border : 1px solid #ccc;
    margin : 0 auto;
    border-radius : 15px;
}

#eyeglass {
    height : 530px;
}

#eyeglass #wrapper {
    position: relative;
    height: auto;
    margin: 0 auto;
    float: left;
    width : 800px;
}

#mitnews
{ 
    font-weight : normal;
    margin-top : 20px;
    font-size : 12px;
    width : 220px;
}

#mitnews a {
    font-weight : normal;
}
</style>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-23931362-6', 'auto');
  ga('send', 'pageview');

</script>

</head>

<body data-new-gr-c-s-check-loaded="14.1039.0" data-gr-ext-installed="">

<div class="content">

<h1>Domain Generalizing DINO for Visual Regression via Latent Distractor Subspace Consistency</h1>
<p id="authors">
<a href=mailto:nikhil.jangamreddy@uqidar.iitd.ac.in>Nikhil Reddy</a><sup>a,b</sup> 
<a href="">Chetan Arora</a><sup>a </sup> 
<a href="">Mahsa Baktashmotlagh</a><sup>b </sup> 
<br>
<br>
<sup> a </sup> Indian Institute of Technology, Delhi <br>
<sup> b </sup> The University of Queensland, Australia.
</p>

<div class="downloadpaper">
<a href="architechture.pdf"><img src="architechture.pdf" width="400px" border="2"></a>
</div>

<p>Vision Foundation Models, such as \dino~\cite{Dinov2}, have demonstrated remarkable generalization in classification; however, their application to out-of-domain visual regression tasks remains a significant and underexplored challenge. Unlike classification, domain generalization in regression poses distinct challenges: regression produces continuous outputs and is particularly sensitive to high-variance, label-irrelevant factors (e.g., illumination, blur, or contrast). These factors can entangle with task-relevant features and induce spurious correlations. While recent regression methods~\cite{c-mixup,ranksim,circe,fds,conr} have shown promise, they often rely on CNN backbones and require the pre-specification of known distractors. This demands significant domain expertise and fails to address spurious correlations that emerge during training. To address these challenges, we propose \proposedapproach, a \textbf{L}atent \textbf{D}istractor \textbf{S}ubspace \textbf{C}onsistency framework that disentangles intermediate feature representation into task-relevant and latent distractor subspaces, and regularizes the latter under photometric perturbations to suppress spurious correlations while preserving discriminative features during training. Our proposed method, \proposedapproach, is the first to effectively adapt the powerful \dino backbone for domain generalized visual regression. \proposedapproach achieves state-of-the-art results on seven benchmark regression datasets, demonstrating its strong performance in domain generalization for visual regression with percentage improvements of (41.75\%, 20.12\%, 52.05\%, 8.27\%, 22.21\%, 3.55\%) over state-of-the-art \DG regression methods, respectively. 
</p>

<p><a href=2312.pdf" target="_blank">Paper</a></a></p>


<br clear="all">
</div>







<!--div class="content" id="references">

<h2>Reference</h2>

<p>B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).</p>

<code>
@article{zhou2015cnnlocalization,<br>
&nbsp;&nbsp;title={{Learning Deep Features for Discriminative Localization.}},<br>
&nbsp;&nbsp;author={Zhou, B. and Khosla, A. and Lapedriza. A. and Oliva, A. and Torralba, A.},<br>
&nbsp;&nbsp;journal={CVPR},<br>
&nbsp;&nbsp;year={2016}<br>
}
</code>

<p>Acknowledgement: <br>This work was supported by NSF grant IIS-1524817, and by a Google faculty research award to A.T.</p>

<p></p><center><a href="https://accessibility.mit.edu/"><b>Accessibility</b></a></center><p></p>
</div-->

</body>
</html>
